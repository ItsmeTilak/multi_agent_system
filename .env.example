# LLM Configuration - OpenRouter
LLM_MODEL_NAME=meta-llama/llama-4-maverick:free
LLM_API_BASE=https://openrouter.ai/api/v1
LLM_API_KEY=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
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
LLM_TIMEOUT=30

# Alternative: Ollama Configuration (if using local LLM)
# LLM_MODEL_NAME=llama-2-7b-chat
# LLM_API_BASE=http://localhost:11434
# LLM_API_KEY=

# Optional: OpenAI Configuration (if using OpenAI instead of local LLM)
# OPENAI_API_KEY=your_openai_api_key

# Optional: Anthropic Configuration (if using Claude)
# ANTHROPIC_API_KEY=your_anthropic_api_key

# Application Settings
DEBUG=False
MOCK_LLM=False
LOG_LEVEL=INFO

# Streamlit Configuration
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=localhost